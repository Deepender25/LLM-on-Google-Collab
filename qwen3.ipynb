{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83e\udde0 Qwen3-1.7B \u2014 Chat with Real-Time Thinking\n",
                "\n",
                "Run all cells to launch a web UI. A **public link** will appear at the bottom \u2014 click it to start chatting!\n",
                "\n",
                "> **Requirements**: Google Colab with **T4 GPU** runtime (free tier)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 \u2014 Install Dependencies\n",
                "!pip install -q transformers accelerate gradio torch bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u23f3 Loading tokenizer for Qwen/Qwen3-1.7B...\n",
                        "\u23f3 Loading model Qwen/Qwen3-1.7B with 4-bit quantization...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3ac9e9e519f44428a013a15ba31104d7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Model loaded! GPU memory used: 2.6 GB\n"
                    ]
                }
            ],
            "source": [
                "# Cell 2 \u2014 Load Model & Tokenizer\n",
                "import torch\n",
                "import gc\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig\n",
                "\n",
                "# Clear any leftover GPU memory from previous runs\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
                "\n",
                "# 4-bit quantization to save GPU memory\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                ")\n",
                "\n",
                "print(f\"\u23f3 Loading tokenizer for {MODEL_ID}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "\n",
                "print(f\"\u23f3 Loading model {MODEL_ID} with 4-bit quantization...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "print(f\"\u2705 Model loaded! GPU memory used: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Chat logic ready.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3 \u2014 Chat Logic with Thinking Parser\n",
                "import threading\n",
                "import re\n",
                "\n",
                "def respond(message, history, enable_thinking=True):\n",
                "    \"\"\"\n",
                "    Generator that streams the model's response.\n",
                "    Parses <think>...</think> tags to separate thinking from the answer.\n",
                "    Yields (thinking_text, answer_text) tuples as tokens arrive.\n",
                "    \"\"\"\n",
                "    # Build conversation messages\n",
                "    messages = []\n",
                "    for user_msg, bot_msg in history:\n",
                "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "        if bot_msg:\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "    messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "    # Apply chat template with thinking enabled\n",
                "    text = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True,\n",
                "        enable_thinking=enable_thinking,\n",
                "    )\n",
                "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "    # Set up streamer\n",
                "    streamer = TextIteratorStreamer(\n",
                "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
                "    )\n",
                "\n",
                "    # Generation config\n",
                "    gen_kwargs = dict(\n",
                "        **inputs,\n",
                "        max_new_tokens=8192,\n",
                "        streamer=streamer,\n",
                "        temperature=0.6,\n",
                "        top_p=0.95,\n",
                "        top_k=20,\n",
                "        do_sample=True,\n",
                "    )\n",
                "\n",
                "    # Start generation in a separate thread\n",
                "    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)\n",
                "    thread.start()\n",
                "\n",
                "    # Stream tokens and parse thinking vs answer\n",
                "    full_response = \"\"\n",
                "    thinking_text = \"\"\n",
                "    answer_text = \"\"\n",
                "    in_thinking = False\n",
                "    thinking_done = False\n",
                "\n",
                "    for new_token in streamer:\n",
                "        full_response += new_token\n",
                "        \n",
                "        # If thinking is disabled, treat everything as answer\n",
                "        if not enable_thinking:\n",
                "             answer_text = full_response\n",
                "             yield \"\", answer_text\n",
                "             continue\n",
                "\n",
                "        if not thinking_done:\n",
                "            # Check if we've entered the thinking block\n",
                "            if \"<think>\" in full_response and not in_thinking:\n",
                "                in_thinking = True\n",
                "\n",
                "            # Check if thinking block has ended\n",
                "            if \"</think>\" in full_response and in_thinking:\n",
                "                thinking_done = True\n",
                "                in_thinking = False\n",
                "                # Extract thinking content\n",
                "                think_match = re.search(r\"<think>(.*?)</think>\", full_response, re.DOTALL)\n",
                "                if think_match:\n",
                "                    thinking_text = think_match.group(1).strip()\n",
                "                # Extract answer (everything after </think>)\n",
                "                answer_text = full_response.split(\"</think>\")[-1].strip()\n",
                "                yield thinking_text, answer_text\n",
                "                continue\n",
                "\n",
                "            if in_thinking:\n",
                "                # Extract partial thinking (remove the <think> tag)\n",
                "                partial = full_response.split(\"<think>\")[-1].strip()\n",
                "                thinking_text = partial\n",
                "                yield thinking_text, \"\"\n",
                "            else:\n",
                "                # Before <think> tag appears, show as answer\n",
                "                answer_text = full_response.strip()\n",
                "                yield \"\", answer_text\n",
                "        else:\n",
                "            # After thinking is done, everything is the answer\n",
                "            answer_text = full_response.split(\"</think>\")[-1].strip()\n",
                "            yield thinking_text, answer_text\n",
                "\n",
                "    thread.join()\n",
                "\n",
                "print(\"\u2705 Chat logic ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipython-input-1928717394.py:129: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
                        "  with gr.Blocks(\n",
                        "/tmp/ipython-input-1928717394.py:129: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
                        "  with gr.Blocks(\n",
                        "/tmp/ipython-input-1928717394.py:140: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
                        "  chatbot = gr.Chatbot(\n",
                        "/tmp/ipython-input-1928717394.py:140: DeprecationWarning: The 'show_copy_button' parameter will be removed in Gradio 6.0. You will need to use 'buttons=[\"copy\"]' instead.\n",
                        "  chatbot = gr.Chatbot(\n",
                        "/tmp/ipython-input-1928717394.py:140: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
                        "  chatbot = gr.Chatbot(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\ud83d\ude80 Launching Gradio UI...\n",
                        "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
                        "* Running on public URL: https://cbc2c1dac4eef83ac4.gradio.live\n",
                        "\n",
                        "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div><iframe src=\"https://cbc2c1dac4eef83ac4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": []
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Cell 4 \u2014 Gradio Web UI\n",
                "import gradio as gr\n",
                "\n",
                "CUSTOM_CSS = \"\"\"\n",
                "/* \u2500\u2500 Global \u2500\u2500 */\n",
                ".gradio-container {\n",
                "    max-width: 900px !important;\n",
                "    margin: 0 auto !important;\n",
                "    font-family: 'Inter', 'Segoe UI', sans-serif !important;\n",
                "}\n",
                "\n",
                "/* \u2500\u2500 Title \u2500\u2500 */\n",
                ".title-text {\n",
                "    text-align: center;\n",
                "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
                "    -webkit-background-clip: text;\n",
                "    -webkit-text-fill-color: transparent;\n",
                "    font-size: 2rem;\n",
                "    font-weight: 800;\n",
                "    margin-bottom: 0;\n",
                "}\n",
                "\n",
                "/* \u2500\u2500 Thinking Panel \u2500\u2500 */\n",
                ".thinking-box {\n",
                "    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%) !important;\n",
                "    border: 1px solid #334155 !important;\n",
                "    border-radius: 12px !important;\n",
                "    padding: 16px !important;\n",
                "    font-size: 0.9rem !important;\n",
                "    line-height: 1.6 !important;\n",
                "    color: #94a3b8 !important;\n",
                "    max-height: 350px !important;\n",
                "    overflow-y: auto !important;\n",
                "}\n",
                "\n",
                "/* \u2500\u2500 Answer Panel \u2500\u2500 */\n",
                ".answer-box {\n",
                "    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%) !important;\n",
                "    border: 1px solid #475569 !important;\n",
                "    border-radius: 12px !important;\n",
                "    padding: 16px !important;\n",
                "    font-size: 1rem !important;\n",
                "    line-height: 1.7 !important;\n",
                "    color: #e2e8f0 !important;\n",
                "    min-height: 80px !important;\n",
                "}\n",
                "\n",
                "/* \u2500\u2500 Chat History \u2500\u2500 */\n",
                ".chat-history {\n",
                "    border: 1px solid #334155 !important;\n",
                "    border-radius: 12px !important;\n",
                "    max-height: 400px !important;\n",
                "    overflow-y: auto !important;\n",
                "}\n",
                "\n",
                "/* \u2500\u2500 Input Box \u2500\u2500 */\n",
                ".input-box textarea {\n",
                "    border-radius: 12px !important;\n",
                "    border: 1px solid #475569 !important;\n",
                "    font-size: 1rem !important;\n",
                "}\n",
                "\n",
                "/* \u2500\u2500 Buttons \u2500\u2500 */\n",
                ".send-btn {\n",
                "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;\n",
                "    border: none !important;\n",
                "    border-radius: 12px !important;\n",
                "    color: white !important;\n",
                "    font-weight: 600 !important;\n",
                "    min-height: 45px !important;\n",
                "}\n",
                ".send-btn:hover {\n",
                "    opacity: 0.9 !important;\n",
                "    transform: translateY(-1px);\n",
                "}\n",
                ".clear-btn {\n",
                "    border-radius: 12px !important;\n",
                "    min-height: 45px !important;\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "def user_submit(message, history):\n",
                "    \"\"\"Add user message to history and clear input.\"\"\"\n",
                "    history = history + [[message, None]]\n",
                "    return \"\", history\n",
                "\n",
                "def bot_respond(history, enable_thinking):\n",
                "    \"\"\"\n",
                "    Generator: streams thinking + answer and updates Gradio components live.\n",
                "    \"\"\"\n",
                "    if not history:\n",
                "        yield history, \"\", \"\"\n",
                "        return\n",
                "\n",
                "    user_message = history[-1][0]\n",
                "    # Build clean history (exclude latest)\n",
                "    clean_history = [(h[0], h[1] or \"\") for h in history[:-1]]\n",
                "\n",
                "    thinking = \"\"\n",
                "    answer = \"\"\n",
                "\n",
                "    for thinking_chunk, answer_chunk in respond(user_message, clean_history, enable_thinking):\n",
                "        thinking = thinking_chunk\n",
                "        answer = answer_chunk\n",
                "\n",
                "        # Update bot message in history with the answer\n",
                "        history[-1][1] = answer if answer else \"\u23f3 Thinking...\"\n",
                "\n",
                "        # Format thinking display\n",
                "        thinking_display = \"\"\n",
                "        if thinking:\n",
                "            thinking_display = f\"\ud83d\udcad **Model's Reasoning:**\\n\\n{thinking}\"\n",
                "\n",
                "        yield history, thinking_display, answer\n",
                "\n",
                "    # Final update\n",
                "    if not answer:\n",
                "        history[-1][1] = thinking if thinking else \"(No response generated)\"\n",
                "    else:\n",
                "        history[-1][1] = answer\n",
                "\n",
                "    thinking_display = \"\"\n",
                "    if thinking:\n",
                "        thinking_display = f\"\ud83d\udcad **Model's Reasoning:**\\n\\n{thinking}\"\n",
                "\n",
                "    yield history, thinking_display, answer\n",
                "\n",
                "\n",
                "with gr.Blocks(\n",
                "    theme=gr.themes.Soft(primary_hue=\"purple\", neutral_hue=\"slate\"),\n",
                "    css=CUSTOM_CSS,\n",
                "    title=\"Qwen3 Chat\",\n",
                ") as demo:\n",
                "\n",
                "    gr.HTML(\"<h1 class='title-text'>\ud83e\udde0 Qwen3-1.7B Chat</h1>\")\n",
                "    gr.HTML(\"<p style='text-align:center;color:#94a3b8;margin-top:-8px;'>Chat with real-time thinking \u00b7 Powered by Qwen3</p>\")\n",
                "\n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=3):\n",
                "            chatbot = gr.Chatbot(\n",
                "                label=\"\ud83d\udcac Conversation\",\n",
                "                height=420,\n",
                "                elem_classes=[\"chat-history\"],\n",
                "                show_copy_button=True,\n",
                "            )\n",
                "            with gr.Row():\n",
                "                msg_input = gr.Textbox(\n",
                "                    placeholder=\"Type your message here... (Enter to send)\",\n",
                "                    show_label=False,\n",
                "                    scale=5,\n",
                "                    elem_classes=[\"input-box\"],\n",
                "                    lines=1,\n",
                "                )\n",
                "                send_btn = gr.Button(\"Send \u25b6\", scale=1, elem_classes=[\"send-btn\"])\n",
                "                clear_btn = gr.Button(\"\ud83d\uddd1\ufe0f Clear\", scale=1, elem_classes=[\"clear-btn\"])\n",
                "                enable_thinking_chk = gr.Checkbox(label=\"Thinking\", value=True, interactive=True, scale=1)\n",
                "            \n",
                "\n",
                "        with gr.Column(scale=2):\n",
                "            with gr.Accordion(\"\ud83d\udd0d Thinking Process\", open=True):\n",
                "                thinking_display = gr.Markdown(\n",
                "                    value=\"*Thinking will appear here in real-time as the model reasons...*\",\n",
                "                    elem_classes=[\"thinking-box\"],\n",
                "                )\n",
                "            with gr.Accordion(\"\ud83d\udcdd Final Answer\", open=True):\n",
                "                answer_display = gr.Markdown(\n",
                "                    value=\"*The model's answer will appear here...*\",\n",
                "                    elem_classes=[\"answer-box\"],\n",
                "                )\n",
                "\n",
                "    # \u2500\u2500 Event Handlers \u2500\u2500\n",
                "    # Submit on Enter\n",
                "    msg_input.submit(\n",
                "        fn=user_submit,\n",
                "        inputs=[msg_input, chatbot],\n",
                "        outputs=[msg_input, chatbot],\n",
                "    ).then(\n",
                "        fn=bot_respond,\n",
                "        inputs=[chatbot, enable_thinking_chk],\n",
                "        outputs=[chatbot, thinking_display, answer_display],\n",
                "    )\n",
                "\n",
                "    # Submit on button click\n",
                "    send_btn.click(\n",
                "        fn=user_submit,\n",
                "        inputs=[msg_input, chatbot],\n",
                "        outputs=[msg_input, chatbot],\n",
                "    ).then(\n",
                "        fn=bot_respond,\n",
                "        inputs=[chatbot, enable_thinking_chk],\n",
                "        outputs=[chatbot, thinking_display, answer_display],\n",
                "    )\n",
                "\n",
                "    # Clear button\n",
                "    clear_btn.click(\n",
                "        fn=lambda: ([], \"*Thinking will appear here...*\", \"*Answer will appear here...*\"),\n",
                "        outputs=[chatbot, thinking_display, answer_display],\n",
                "    )\n",
                "\n",
                "print(\"\ud83d\ude80 Launching Gradio UI...\")\n",
                "demo.launch(share=True, debug=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## \u26d4 Stop the Chatbot\n",
                "Run the cell below to **stop** the chatbot server and free GPU memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Closing server running on port: 7860\n",
                        "\ud83d\uded1 Chatbot server stopped.\n",
                        "\ud83d\udca1 To restart, just re-run Cell 4 above.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 5 \u2014 Stop the chatbot server\n",
                "demo.close()\n",
                "print(\"\ud83d\uded1 Chatbot server stopped.\")\n",
                "print(\"\ud83d\udca1 To restart, just re-run Cell 4 above.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}