{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Qwen3-1.7B ‚Äî Chat with Real-Time Thinking\n",
                "\n",
                "Run all cells to launch a web UI. A **public link** will appear at the bottom ‚Äî click it to start chatting!\n",
                "\n",
                "> **Requirements**: Google Colab with **T4 GPU** runtime (free tier)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 ‚Äî Install Dependencies\n",
                "!pip install -q transformers accelerate gradio torch bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 ‚Äî Load Model & Tokenizer\n",
                "import torch\n",
                "import gc\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig\n",
                "\n",
                "# Clear any leftover GPU memory from previous runs\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
                "\n",
                "# 4-bit quantization to save GPU memory\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                ")\n",
                "\n",
                "print(f\"‚è≥ Loading tokenizer for {MODEL_ID}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "\n",
                "print(f\"‚è≥ Loading model {MODEL_ID} with 4-bit quantization...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Model loaded! GPU memory used: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3 ‚Äî Chat Logic with Thinking Parser\n",
                "import threading\n",
                "import re\n",
                "\n",
                "def respond(message, history):\n",
                "    \"\"\"\n",
                "    Generator that streams the model's response.\n",
                "    Parses <think>...</think> tags to separate thinking from the answer.\n",
                "    Yields (thinking_text, answer_text) tuples as tokens arrive.\n",
                "    \"\"\"\n",
                "    # Build conversation messages\n",
                "    messages = []\n",
                "    for user_msg, bot_msg in history:\n",
                "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "        if bot_msg:\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "    messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "    # Apply chat template with thinking enabled\n",
                "    text = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True,\n",
                "        enable_thinking=True,\n",
                "    )\n",
                "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "    # Set up streamer\n",
                "    streamer = TextIteratorStreamer(\n",
                "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
                "    )\n",
                "\n",
                "    # Generation config\n",
                "    gen_kwargs = dict(\n",
                "        **inputs,\n",
                "        max_new_tokens=8192,\n",
                "        streamer=streamer,\n",
                "        temperature=0.6,\n",
                "        top_p=0.95,\n",
                "        top_k=20,\n",
                "        do_sample=True,\n",
                "    )\n",
                "\n",
                "    # Start generation in a separate thread\n",
                "    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)\n",
                "    thread.start()\n",
                "\n",
                "    # Stream tokens and parse thinking vs answer\n",
                "    full_response = \"\"\n",
                "    thinking_text = \"\"\n",
                "    answer_text = \"\"\n",
                "    in_thinking = False\n",
                "    thinking_done = False\n",
                "\n",
                "    for new_token in streamer:\n",
                "        full_response += new_token\n",
                "\n",
                "        if not thinking_done:\n",
                "            # Check if we've entered the thinking block\n",
                "            if \"<think>\" in full_response and not in_thinking:\n",
                "                in_thinking = True\n",
                "\n",
                "            # Check if thinking block has ended\n",
                "            if \"</think>\" in full_response and in_thinking:\n",
                "                thinking_done = True\n",
                "                in_thinking = False\n",
                "                # Extract thinking content\n",
                "                think_match = re.search(r\"<think>(.*?)</think>\", full_response, re.DOTALL)\n",
                "                if think_match:\n",
                "                    thinking_text = think_match.group(1).strip()\n",
                "                # Extract answer (everything after </think>)\n",
                "                answer_text = full_response.split(\"</think>\")[-1].strip()\n",
                "                yield thinking_text, answer_text\n",
                "                continue\n",
                "\n",
                "            if in_thinking:\n",
                "                # Extract partial thinking (remove the <think> tag)\n",
                "                partial = full_response.split(\"<think>\")[-1].strip()\n",
                "                thinking_text = partial\n",
                "                yield thinking_text, \"\"\n",
                "            else:\n",
                "                # Before <think> tag appears, show as answer\n",
                "                answer_text = full_response.strip()\n",
                "                yield \"\", answer_text\n",
                "        else:\n",
                "            # After thinking is done, everything is the answer\n",
                "            answer_text = full_response.split(\"</think>\")[-1].strip()\n",
                "            yield thinking_text, answer_text\n",
                "\n",
                "    thread.join()\n",
                "\n",
                "print(\"‚úÖ Chat logic ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4 ‚Äî Gradio Web UI\n",
                "import gradio as gr\n",
                "\n",
                "CUSTOM_CSS = \"\"\"\n",
                "/* ‚îÄ‚îÄ Global ‚îÄ‚îÄ */\n",
                ".gradio-container {\n",
                "    max-width: 900px !important;\n",
                "    margin: 0 auto !important;\n",
                "    font-family: 'Inter', 'Segoe UI', sans-serif !important;\n",
                "}\n",
                "\n",
                "/* ‚îÄ‚îÄ Title ‚îÄ‚îÄ */\n",
                ".title-text {\n",
                "    text-align: center;\n",
                "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
                "    -webkit-background-clip: text;\n",
                "    -webkit-text-fill-color: transparent;\n",
                "    font-size: 2rem;\n",
                "    font-weight: 800;\n",
                "    margin-bottom: 0;\n",
                "}\n",
                "\n",
                "/* ‚îÄ‚îÄ Thinking Panel ‚îÄ‚îÄ */\n",
                ".thinking-box {\n",
                "    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%) !important;\n",
                "    border: 1px solid #334155 !important;\n",
                "    border-radius: 12px !important;\n",
                "    padding: 16px !important;\n",
                "    font-size: 0.9rem !important;\n",
                "    line-height: 1.6 !important;\n",
                "    color: #94a3b8 !important;\n",
                "    max-height: 350px !important;\n",
                "    overflow-y: auto !important;\n",
                "}\n",
                "\n",
                "/* ‚îÄ‚îÄ Answer Panel ‚îÄ‚îÄ */\n",
                ".answer-box {\n",
                "    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%) !important;\n",
                "    border: 1px solid #475569 !important;\n",
                "    border-radius: 12px !important;\n",
                "    padding: 16px !important;\n",
                "    font-size: 1rem !important;\n",
                "    line-height: 1.7 !important;\n",
                "    color: #e2e8f0 !important;\n",
                "    min-height: 80px !important;\n",
                "}\n",
                "\n",
                "/* ‚îÄ‚îÄ Chat History ‚îÄ‚îÄ */\n",
                ".chat-history {\n",
                "    border: 1px solid #334155 !important;\n",
                "    border-radius: 12px !important;\n",
                "    max-height: 400px !important;\n",
                "    overflow-y: auto !important;\n",
                "}\n",
                "\n",
                "/* ‚îÄ‚îÄ Input Box ‚îÄ‚îÄ */\n",
                ".input-box textarea {\n",
                "    border-radius: 12px !important;\n",
                "    border: 1px solid #475569 !important;\n",
                "    font-size: 1rem !important;\n",
                "}\n",
                "\n",
                "/* ‚îÄ‚îÄ Buttons ‚îÄ‚îÄ */\n",
                ".send-btn {\n",
                "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;\n",
                "    border: none !important;\n",
                "    border-radius: 12px !important;\n",
                "    color: white !important;\n",
                "    font-weight: 600 !important;\n",
                "    min-height: 45px !important;\n",
                "}\n",
                ".send-btn:hover {\n",
                "    opacity: 0.9 !important;\n",
                "    transform: translateY(-1px);\n",
                "}\n",
                ".clear-btn {\n",
                "    border-radius: 12px !important;\n",
                "    min-height: 45px !important;\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "def user_submit(message, history):\n",
                "    \"\"\"Add user message to history and clear input.\"\"\"\n",
                "    history = history + [[message, None]]\n",
                "    return \"\", history\n",
                "\n",
                "def bot_respond(history):\n",
                "    \"\"\"\n",
                "    Generator: streams thinking + answer and updates Gradio components live.\n",
                "    \"\"\"\n",
                "    if not history:\n",
                "        yield history, \"\", \"\"\n",
                "        return\n",
                "\n",
                "    user_message = history[-1][0]\n",
                "    # Build clean history (exclude latest)\n",
                "    clean_history = [(h[0], h[1] or \"\") for h in history[:-1]]\n",
                "\n",
                "    thinking = \"\"\n",
                "    answer = \"\"\n",
                "\n",
                "    for thinking_chunk, answer_chunk in respond(user_message, clean_history):\n",
                "        thinking = thinking_chunk\n",
                "        answer = answer_chunk\n",
                "\n",
                "        # Update bot message in history with the answer\n",
                "        history[-1][1] = answer if answer else \"‚è≥ Thinking...\"\n",
                "\n",
                "        # Format thinking display\n",
                "        thinking_display = \"\"\n",
                "        if thinking:\n",
                "            thinking_display = f\"üí≠ **Model's Reasoning:**\\n\\n{thinking}\"\n",
                "\n",
                "        yield history, thinking_display, answer\n",
                "\n",
                "    # Final update\n",
                "    if not answer:\n",
                "        history[-1][1] = thinking if thinking else \"(No response generated)\"\n",
                "    else:\n",
                "        history[-1][1] = answer\n",
                "\n",
                "    thinking_display = \"\"\n",
                "    if thinking:\n",
                "        thinking_display = f\"üí≠ **Model's Reasoning:**\\n\\n{thinking}\"\n",
                "\n",
                "    yield history, thinking_display, answer\n",
                "\n",
                "\n",
                "with gr.Blocks(\n",
                "    theme=gr.themes.Soft(primary_hue=\"purple\", neutral_hue=\"slate\"),\n",
                "    css=CUSTOM_CSS,\n",
                "    title=\"Qwen3 Chat\",\n",
                ") as demo:\n",
                "\n",
                "    gr.HTML(\"<h1 class='title-text'>üß† Qwen3-1.7B Chat</h1>\")\n",
                "    gr.HTML(\"<p style='text-align:center;color:#94a3b8;margin-top:-8px;'>Chat with real-time thinking ¬∑ Powered by Qwen3</p>\")\n",
                "\n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=3):\n",
                "            chatbot = gr.Chatbot(\n",
                "                label=\"üí¨ Conversation\",\n",
                "                height=420,\n",
                "                elem_classes=[\"chat-history\"],\n",
                "                show_copy_button=True,\n",
                "            )\n",
                "            with gr.Row():\n",
                "                msg_input = gr.Textbox(\n",
                "                    placeholder=\"Type your message here... (Enter to send)\",\n",
                "                    show_label=False,\n",
                "                    scale=5,\n",
                "                    elem_classes=[\"input-box\"],\n",
                "                    lines=1,\n",
                "                )\n",
                "                send_btn = gr.Button(\"Send ‚ñ∂\", scale=1, elem_classes=[\"send-btn\"])\n",
                "                clear_btn = gr.Button(\"üóëÔ∏è Clear\", scale=1, elem_classes=[\"clear-btn\"])\n",
                "\n",
                "        with gr.Column(scale=2):\n",
                "            with gr.Accordion(\"üîç Thinking Process\", open=True):\n",
                "                thinking_display = gr.Markdown(\n",
                "                    value=\"*Thinking will appear here in real-time as the model reasons...*\",\n",
                "                    elem_classes=[\"thinking-box\"],\n",
                "                )\n",
                "            with gr.Accordion(\"üìù Final Answer\", open=True):\n",
                "                answer_display = gr.Markdown(\n",
                "                    value=\"*The model's answer will appear here...*\",\n",
                "                    elem_classes=[\"answer-box\"],\n",
                "                )\n",
                "\n",
                "    # ‚îÄ‚îÄ Event Handlers ‚îÄ‚îÄ\n",
                "    # Submit on Enter\n",
                "    msg_input.submit(\n",
                "        fn=user_submit,\n",
                "        inputs=[msg_input, chatbot],\n",
                "        outputs=[msg_input, chatbot],\n",
                "    ).then(\n",
                "        fn=bot_respond,\n",
                "        inputs=[chatbot],\n",
                "        outputs=[chatbot, thinking_display, answer_display],\n",
                "    )\n",
                "\n",
                "    # Submit on button click\n",
                "    send_btn.click(\n",
                "        fn=user_submit,\n",
                "        inputs=[msg_input, chatbot],\n",
                "        outputs=[msg_input, chatbot],\n",
                "    ).then(\n",
                "        fn=bot_respond,\n",
                "        inputs=[chatbot],\n",
                "        outputs=[chatbot, thinking_display, answer_display],\n",
                "    )\n",
                "\n",
                "    # Clear button\n",
                "    clear_btn.click(\n",
                "        fn=lambda: ([], \"*Thinking will appear here...*\", \"*Answer will appear here...*\"),\n",
                "        outputs=[chatbot, thinking_display, answer_display],\n",
                "    )\n",
                "\n",
                "print(\"üöÄ Launching Gradio UI...\")\n",
                "demo.launch(share=True, debug=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ‚õî Stop the Chatbot\n",
                "Run the cell below to **stop** the chatbot server and free GPU memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5 ‚Äî Stop the chatbot server\n",
                "demo.close()\n",
                "print(\"üõë Chatbot server stopped.\")\n",
                "print(\"üí° To restart, just re-run Cell 4 above.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
